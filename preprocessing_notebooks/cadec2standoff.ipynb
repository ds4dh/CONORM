{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98abc19b-7e3a-4d02-9e77-8b934ef64694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def build_en_dict_from_MedDRA(path2lltasc: str, path2ptasc: str) -> None:\n",
    "    \"\"\"\n",
    "    Builds a dictionary from MedDRA llt and pt files.\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(path2ptasc):\n",
    "        print(\"Error: Folder Not Found \", path2ptasc)\n",
    "        return\n",
    "\n",
    "    pt_dict = {}\n",
    "    pt_to_hlt = {}\n",
    "\n",
    "    with open(path2ptasc, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            fs = line.strip().split(\"$\")\n",
    "            pt = fs[0]\n",
    "            text = fs[1]\n",
    "            hlt = fs[2]\n",
    "\n",
    "            if pt not in pt_dict:\n",
    "                pt_dict[pt] = text\n",
    "            else:\n",
    "                print(\"0\")\n",
    "                \n",
    "            pt_to_hlt[pt] = hlt\n",
    "\n",
    "    if not os.path.exists(path2lltasc):\n",
    "        print(\"Error: Folder Not Found \", path2lltasc)\n",
    "        return\n",
    "\n",
    "    llt_dict = {}\n",
    "    llt_to_pt = {}\n",
    "\n",
    "    with open(path2lltasc, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            fs = line.strip().split(\"$\")\n",
    "            llt = fs[0]\n",
    "            text = fs[1]\n",
    "            pt = fs[2]\n",
    "\n",
    "            if llt not in llt_dict:\n",
    "                llt_dict[llt] = text\n",
    "            else:\n",
    "                print(\"1\")\n",
    "                \n",
    "            llt_to_pt[llt] = pt\n",
    "            \n",
    "    return llt_dict, llt_to_pt, pt_dict\n",
    "\n",
    "def is_whitespace_string(s):\n",
    "    for char in s:\n",
    "        if not char.isspace():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def tuple_list_to_string_pairs(tuple_list):\n",
    "    string_pairs = [f\"{x} {y}\" for x, y in tuple_list]\n",
    "    return string_pairs\n",
    "\n",
    "def spans_overlap(span1, span2):\n",
    "    return max(span1[0], span2[0]) < min(span1[1], span2[1])\n",
    "\n",
    "def process_files(files,\n",
    "                  path_to_ann,\n",
    "                  path_to_txt,\n",
    "                  path_to_meddra,\n",
    "                  output_folder,\n",
    "                  split_type,\n",
    "                  consider_discontinuous,\n",
    "                  llt_dict,\n",
    "                  llt_to_pt,\n",
    "                  pt_dict):\n",
    "    \n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    continuous_counter = 0\n",
    "    discontinuous_counter = 0\n",
    "    overlap_counter = 0\n",
    "\n",
    "    for file_name in tqdm(files):\n",
    "        \n",
    "        ADE_counter = 0\n",
    "        NORM_counter = 0\n",
    "        continuous_ADE_spans = []\n",
    "        \n",
    "        ann_file_path = os.path.join(path_to_ann, file_name + '.ann')\n",
    "        txt_file_path = os.path.join(path_to_txt, file_name + '.txt')\n",
    "        meddra_file_path = os.path.join(path_to_meddra, file_name + '.ann')\n",
    "\n",
    "        with open(txt_file_path, 'r') as file:\n",
    "            txt_content = file.read()\n",
    "\n",
    "        normalization_data = {}\n",
    "        with open(meddra_file_path, 'r') as meddra_file:\n",
    "            for line in meddra_file:\n",
    "                if line.startswith('TT'):\n",
    "                    try:\n",
    "                        norm_id, norm_infos, _ = line.strip().split('\\t')\n",
    "                        norm_infos = norm_infos.replace(\" + \", \"/\").split()[0].split(\"/\")\n",
    "                        processed_norms = []\n",
    "                        for norm in norm_infos:\n",
    "                            if norm in llt_dict:\n",
    "                                processed_norms.append(norm)\n",
    "                        adr_id = norm_id.replace('TT', 'T')\n",
    "                        normalization_data[adr_id] = processed_norms\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "        T_lines = []\n",
    "        N_lines = []\n",
    "        \n",
    "        with open(ann_file_path, 'r') as ann_file:\n",
    "            for line in ann_file:\n",
    "                if line.startswith('T'):\n",
    "                    line_split = line.strip().split('\\t')\n",
    "                    if len(line_split) >= 3:\n",
    "                        ann_number, ann_info, text = line_split[:3]\n",
    "                        ann_split = ann_info.split(' ')\n",
    "                        ann_type = ann_split[0]\n",
    "                        ann_spans = \" \".join(ann_split[1:]).split(\";\")\n",
    "                        spans = [(int(part.split()[0]), int(part.split()[1])) for part in ann_spans]\n",
    "                        sorted_spans = sorted(spans, key=lambda x: x[0])\n",
    "\n",
    "                        merged_spans = []\n",
    "                        start_span = sorted_spans[0][0]\n",
    "                        end_span = sorted_spans[0][1]\n",
    "\n",
    "                        for i in range(1, len(sorted_spans)):\n",
    "                            if is_whitespace_string(txt_content[end_span:sorted_spans[i][0]]):\n",
    "                                end_span = sorted_spans[i][1]\n",
    "                            else:\n",
    "                                merged_spans.append((start_span, end_span))\n",
    "                                start_span, end_span = sorted_spans[i]\n",
    "\n",
    "                        merged_spans.append((start_span, end_span))\n",
    "\n",
    "                        if ann_type == 'ADR':\n",
    "                            if len(merged_spans) > 1:\n",
    "                                if not consider_discontinuous[split_type]:\n",
    "                                    discontinuous_counter += 1\n",
    "                                    continue\n",
    "                            continuous_ADE_spans.extend(merged_spans)\n",
    "                            continuous_counter += 1 if len(merged_spans) == 1 else 0\n",
    "                            \n",
    "                            ADE_counter += 1\n",
    "                            adr_line = f'T{ADE_counter}\\tADE {\";\".join(tuple_list_to_string_pairs(merged_spans))}\\t{\" \".join([txt_content[s:e] for s, e in merged_spans])}\\n'\n",
    "                            T_lines.append(adr_line)\n",
    "\n",
    "                            if ann_number in normalization_data:\n",
    "                                norm_infos = normalization_data[ann_number]\n",
    "                                for norm_info in norm_infos:\n",
    "                                    pt_memory = []\n",
    "                                    NORM_counter += 1\n",
    "                                    norm_line = f'N{NORM_counter}\\tReference T{ADE_counter} meddra_llt_id:{norm_info}\\t{llt_dict[norm_info]}\\n'\n",
    "                                    N_lines.append(norm_line)\n",
    "                                    if norm_info in llt_to_pt:\n",
    "                                        norm_info_pt = llt_to_pt[norm_info]\n",
    "                                        if norm_info_pt not in pt_memory:\n",
    "                                            NORM_counter += 1\n",
    "                                            norm_line = f'N{NORM_counter}\\tReference T{ADE_counter} meddra_pt_id:{norm_info_pt}\\t{pt_dict[norm_info_pt]}\\n'\n",
    "                                            N_lines.append(norm_line)\n",
    "                                            pt_memory.append(norm_info_pt)\n",
    "\n",
    "        adr_lines = T_lines + N_lines\n",
    "        \n",
    "        # Check for overlaps among continuous ADE spans\n",
    "        if any(spans_overlap(continuous_ADE_spans[i], continuous_ADE_spans[j]) for i in range(len(continuous_ADE_spans)) for j in range(i + 1, len(continuous_ADE_spans))):\n",
    "            overlap_counter += 1\n",
    "            print(f\"Overlap found in {file_name}.ann\")\n",
    "\n",
    "        output_ann_path = os.path.join(output_folder, file_name + '.ann')\n",
    "        with open(output_ann_path, 'w') as file:\n",
    "            file.writelines(adr_lines)\n",
    "\n",
    "        output_txt_path = os.path.join(output_folder, file_name + '.txt')\n",
    "        with open(output_txt_path, 'w') as file:\n",
    "            file.write(txt_content)\n",
    "\n",
    "    print(f\"{split_type} - Discontinuous Counter: {discontinuous_counter} (Ignored if applicable)\")\n",
    "    print(f\"{split_type} - Continuous Counter: {continuous_counter}\")\n",
    "    print(f\"{split_type} - Overlap Counter: {overlap_counter}\")\n",
    "\n",
    "def split_data(path_to_ann,\n",
    "               path_to_txt,\n",
    "               path_to_meddra,\n",
    "               output_base,\n",
    "               train_split,\n",
    "               val_split,\n",
    "               test_split,\n",
    "               seed,\n",
    "               consider_discontinuous,\n",
    "               llt_dict,\n",
    "               llt_to_pt,\n",
    "               pt_dict):\n",
    "\n",
    "    random.seed(seed)\n",
    "\n",
    "    ann_files = {os.path.splitext(file)[0] for file in os.listdir(path_to_ann) if file.endswith('.ann')}\n",
    "    txt_files = {os.path.splitext(file)[0] for file in os.listdir(path_to_txt) if file.endswith('.txt')}\n",
    "\n",
    "    common_files = list(ann_files.intersection(txt_files))\n",
    "    random.shuffle(common_files)\n",
    "\n",
    "    total = len(common_files)\n",
    "    train_end = int(total * train_split)\n",
    "    val_end = train_end + int(total * val_split)\n",
    "\n",
    "    train_files = common_files[:train_end]\n",
    "    val_files = common_files[train_end:val_end]\n",
    "    test_files = common_files[val_end:]\n",
    "\n",
    "    process_files(train_files, path_to_ann, path_to_txt, path_to_meddra, os.path.join(output_base, 'train'), 'train', consider_discontinuous, llt_dict, llt_to_pt, pt_dict)\n",
    "    process_files(val_files, path_to_ann, path_to_txt, path_to_meddra, os.path.join(output_base, 'val'), 'val', consider_discontinuous, llt_dict, llt_to_pt, pt_dict)\n",
    "    process_files(test_files, path_to_ann, path_to_txt, path_to_meddra, os.path.join(output_base, 'test'), 'test', consider_discontinuous, llt_dict, llt_to_pt, pt_dict)\n",
    "\n",
    "\n",
    "llt_dict, llt_to_pt, pt_dict = build_en_dict_from_MedDRA(\n",
    "    \"../../ontology_mapper/meddra_data/CADEC_meddra_16_0_english/MedAscii/llt.asc\",\n",
    "    \"../../ontology_mapper/meddra_data/CADEC_meddra_16_0_english/MedAscii/pt.asc\"\n",
    ")\n",
    "\n",
    "split_data('./data/cadec/original',\n",
    "           './data/cadec/text',\n",
    "           './data/cadec/meddra',\n",
    "           './data/cadec/standoff',\n",
    "           train_split=0.7,\n",
    "           val_split=0.15,\n",
    "           test_split=0.15,\n",
    "           seed=42,\n",
    "           consider_discontinuous={'train': False, 'val': False, 'test': False},\n",
    "           llt_dict=llt_dict,\n",
    "           llt_to_pt=llt_to_pt,\n",
    "           pt_dict=pt_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MultiBioNER",
   "language": "python",
   "name": "multibioner"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
