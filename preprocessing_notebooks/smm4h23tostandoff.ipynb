{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc831ca5-aec1-421d-ac14-1eb05f22c99d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "from typing import Dict, List, Tuple\n",
    "import regex\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "\n",
    "\n",
    "def build_en_dict_from_MedDRA(path2lltasc: str, path2ptasc: str) -> None:\n",
    "    \"\"\"\n",
    "    Builds a dictionary from MedDRA llt and pt files.\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(path2ptasc):\n",
    "        print(\"Error: Folder Not Found \", path2ptasc)\n",
    "        return\n",
    "\n",
    "    pt_dict = {}\n",
    "    pt_to_hlt = {}\n",
    "\n",
    "    with open(path2ptasc, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            fs = line.strip().split(\"$\")\n",
    "            pt = fs[0]\n",
    "            text = fs[1]\n",
    "            hlt = fs[2]\n",
    "\n",
    "            if pt not in pt_dict:\n",
    "                pt_dict[pt] = text\n",
    "            else:\n",
    "                print(\"0\")\n",
    "                \n",
    "            pt_to_hlt[pt] = hlt\n",
    "\n",
    "    if not os.path.exists(path2lltasc):\n",
    "        print(\"Error: Folder Not Found \", path2lltasc)\n",
    "        return\n",
    "\n",
    "    llt_dict = {}\n",
    "    llt_to_pt = {}\n",
    "\n",
    "    with open(path2lltasc, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            fs = line.strip().split(\"$\")\n",
    "            llt = fs[0]\n",
    "            text = fs[1]\n",
    "            pt = fs[2]\n",
    "\n",
    "            if llt not in llt_dict:\n",
    "                llt_dict[llt] = text\n",
    "            else:\n",
    "                print(\"1\")\n",
    "                \n",
    "            llt_to_pt[llt] = pt\n",
    "            \n",
    "    return llt_dict, llt_to_pt, pt_dict\n",
    "\n",
    "def read_tweet_tsv_file(path: str) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Read a TSV file and return its data in a dictionary format.\n",
    "    \"\"\"\n",
    "    data_dict = {}\n",
    "    with open(path, 'r') as tsv_file:\n",
    "        tsv_reader = csv.reader(tsv_file, delimiter='\\t')\n",
    "        for row in tsv_reader:\n",
    "            key = row[0]\n",
    "            value = row[1]\n",
    "            data_dict[key] = value\n",
    "    return data_dict\n",
    "\n",
    "def read_tweet_tsv_file(path: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Read a TSV (Tab-Separated Values) file and return its data in a dictionary format.\n",
    "    Manually processes the file as plain text.\n",
    "    \"\"\"\n",
    "    data_dict = {}\n",
    "    with open(path, 'r', encoding='utf-8') as file:  # Ensure proper encoding is used\n",
    "        text = file.read().strip()  # Read the entire file as a single string, remove trailing newline\n",
    "        rows = text.split('\\n')  # Split the text into rows on newline characters\n",
    "        for row in rows:\n",
    "            parts = row.split('\\t')  # Split each row into parts on tab characters\n",
    "            if len(parts) >= 2:  # Ensure there are at least two parts (key and value)\n",
    "                key, value = parts[0], parts[1]\n",
    "                data_dict[key] = value  # Add to dictionary\n",
    "    return data_dict\n",
    "\n",
    "def read_span_tsv_file(path: str) -> Dict[str, List[Tuple[int, int, str, str]]]:\n",
    "    \"\"\"\n",
    "    Read a TSV file and return its data in a dictionary format.\n",
    "    \"\"\"\n",
    "    data_dict = {}\n",
    "    with open(path, 'r') as tsv_file:\n",
    "        tsv_reader = csv.reader(tsv_file, delimiter='\\t')\n",
    "        for row in tsv_reader:\n",
    "            key = row[0]\n",
    "            span_data = (int(row[2]), int(row[3]), row[4], row[5])  # Start, end, text, meddra_llt\n",
    "            if key not in data_dict:\n",
    "                data_dict[key] = []\n",
    "            data_dict[key].append(span_data)\n",
    "    return data_dict\n",
    "\n",
    "def merge_tweets_spans(tweets: Dict[str, str], spans: Dict[str, List[Tuple[int, int, str, str]]]) -> Dict[str, Dict[str, object]]:\n",
    "    \"\"\"\n",
    "    Merge tweet text and spans into a single dictionary.\n",
    "    \"\"\"\n",
    "    merged_dict = {}\n",
    "    for tweet_id, tweet_text in tweets.items():\n",
    "        merged_dict[tweet_id] = {\n",
    "            'text': tweet_text,\n",
    "            'spans': spans.get(tweet_id, [])\n",
    "        }\n",
    "    return merged_dict\n",
    "\n",
    "def is_within_word(text, start, end):\n",
    "    \"\"\"\n",
    "    Check if the span starts or ends within a word.\n",
    "    - `start` is the start index of the span.\n",
    "    - `end` is the end index of the span.\n",
    "    Returns True if the span starts or ends within a word, False otherwise.\n",
    "    \"\"\"\n",
    "    # Check if the character before the start index is alphanumeric (part of a word)\n",
    "    if start > 0 and text[start-1].isalnum():\n",
    "        return True\n",
    "    # Check if the character after the end index is alphanumeric (part of a word)\n",
    "    if end < len(text) and text[end].isalnum():\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def adjust_span_to_word_boundary(text, start, end):\n",
    "    \"\"\"\n",
    "    Adjusts the span to exclude leading and trailing special characters or spaces,\n",
    "    then aligns it with word boundaries, ensuring indices remain within bounds.\n",
    "    \"\"\"\n",
    "    # Ensure start and end are within bounds\n",
    "    start = max(0, min(start, len(text)))\n",
    "    end = max(0, min(end, len(text)))\n",
    "\n",
    "    # Strip away leading special characters or spaces\n",
    "    while start < end and not text[start].isalnum():\n",
    "        start += 1\n",
    "\n",
    "    # Ensure start is not out of bounds after adjustment\n",
    "    start = min(start, len(text))\n",
    "\n",
    "    # Strip away trailing special characters or spaces\n",
    "    while end > start and not text[end - 1].isalnum():\n",
    "        end -= 1\n",
    "\n",
    "    # Adjust start backwards to the beginning of the word if not already\n",
    "    while start > 0 and text[start - 1].isalnum():\n",
    "        start -= 1\n",
    "\n",
    "    # Adjust end forwards to the end of the word if not already\n",
    "    while end < len(text) and text[end].isalnum():\n",
    "        end += 1\n",
    "\n",
    "    # Ensure end is not out of bounds after adjustment\n",
    "    end = min(end, len(text))\n",
    "\n",
    "    return start, end\n",
    "\n",
    "def generate_standoff_data(data_dict: Dict[str, Dict[str, object]], llt_id_to_txt: Dict[str, str], llt_to_pt: Dict[str, str], pt_dict: Dict[str, str], output_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Generate standoff data files from a dictionary containing tweet text and spans.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    total_t_entities = 0  # Initialize counter for total 'T' entities\n",
    "    for tweet_id, tweet_data in tqdm(data_dict.items()):\n",
    "        text_file_path = os.path.join(output_dir, tweet_id + '.txt')\n",
    "        ann_file_path = os.path.join(output_dir, tweet_id + '.ann')\n",
    "\n",
    "        with open(text_file_path, 'w', encoding='utf-8') as text_file:\n",
    "            cleaned_content = regex.sub('[^\\p{L}\\p{N}\\p{P}]', ' ', tweet_data['text'])\n",
    "            cleaned_content = regex.sub('\\p{Z}', ' ', cleaned_content)\n",
    "            text_file.write(cleaned_content)\n",
    "\n",
    "        t_annotations = []\n",
    "        n_annotations = []\n",
    "        n_counter = 1\n",
    "\n",
    "        for i, span in enumerate(tweet_data['spans']):\n",
    "            # Extract span indices\n",
    "            START, END = span[0], span[1]\n",
    "            if is_within_word(cleaned_content, START, END):\n",
    "                START, END = adjust_span_to_word_boundary(cleaned_content, START, END)\n",
    "            t_line = f\"T{i+1}\\tADE {START} {END}\\t{cleaned_content[START:END]}\"\n",
    "            t_annotations.append(t_line)\n",
    "            total_t_entities += 1  # Increment total 'T' entities counter\n",
    "\n",
    "            if span[3].isdigit() and span[3] in llt_id_to_txt:\n",
    "                meddra_text = llt_id_to_txt[span[3]]\n",
    "                n_line_llt = f\"N{n_counter}\\tReference T{i+1} meddra_llt_id:{span[3]}\\t{meddra_text}\"\n",
    "                n_annotations.append(n_line_llt)\n",
    "                n_counter += 1\n",
    "\n",
    "                if span[3] in llt_to_pt and llt_to_pt[span[3]] in pt_dict:\n",
    "                    pt_id = llt_to_pt[span[3]]\n",
    "                    pt_text = pt_dict[pt_id]\n",
    "                    n_line_pt = f\"N{n_counter}\\tReference T{i+1} meddra_pt_id:{pt_id}\\t{pt_text}\"\n",
    "                    n_annotations.append(n_line_pt)\n",
    "                    n_counter += 1\n",
    "\n",
    "        with open(ann_file_path, 'w', encoding='utf-8') as ann_file:\n",
    "            for t_annotation in t_annotations:\n",
    "                ann_file.write(t_annotation + '\\n')\n",
    "            for n_annotation in n_annotations:\n",
    "                ann_file.write(n_annotation + '\\n')\n",
    "\n",
    "def generate_test_data(tweets: Dict[str, str], output_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Generate text data files for the test set.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for tweet_id, tweet_text in tqdm(tweets.items()):\n",
    "        text_file_path = os.path.join(output_dir, tweet_id + '.txt')\n",
    "\n",
    "        with open(text_file_path, 'w', encoding='utf-8') as text_file:\n",
    "            cleaned_content = regex.sub('[^\\p{L}\\p{N}\\p{P}]', ' ', tweet_text)\n",
    "            cleaned_content = regex.sub('\\p{Z}', ' ', cleaned_content)\n",
    "            text_file.write(cleaned_content)\n",
    "\n",
    "src = \"./data/smm4h23/Task5_train_validation\"\n",
    "train_tweets = read_tweet_tsv_file(os.path.join(src, \"Train/train_tweets.tsv\"))\n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\"\n",
    "    Creating standoff data with new splitting strategy.\n",
    "    \"\"\"\n",
    "    # Set a seed for reproducibility\n",
    "    seed = 42\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Load and prepare data\n",
    "    llt_dict, llt_to_pt, pt_dict = build_en_dict_from_MedDRA(\n",
    "        \"./data/smm4h23/Task5_train_validation/MedDRA/llt.asc\",\n",
    "        \"../../ontology_mapper/meddra_data/SMM4H23_meddra_24.0_english/MedAscii/pt.asc\"\n",
    "    )\n",
    "\n",
    "    src = \"./data/smm4h23/Task5_train_validation\"\n",
    "    dst = \"../data/english/smm4h23\"\n",
    "\n",
    "    # Read train and development datasets\n",
    "    train_tweets = read_tweet_tsv_file(os.path.join(src, \"Train/train_tweets.tsv\"))\n",
    "    train_spans = read_span_tsv_file(os.path.join(src, \"Train/train_spans_norm.tsv\"))\n",
    "    dev_tweets = read_tweet_tsv_file(os.path.join(src, \"Dev/tweets.tsv\"))\n",
    "    dev_spans = read_span_tsv_file(os.path.join(src, \"Dev/spans_norm.tsv\"))\n",
    "\n",
    "    # Merge train dataset and shuffle\n",
    "    merged_train_data = merge_tweets_spans(train_tweets, train_spans)\n",
    "    combined_train_data = list(merged_train_data.items())\n",
    "    random.shuffle(combined_train_data)\n",
    "\n",
    "    # Split into new train and val datasets\n",
    "    split_index = int(0.9 * len(combined_train_data))\n",
    "    new_train_data = dict(combined_train_data[:split_index])\n",
    "    new_val_data = dict(combined_train_data[split_index:])\n",
    "\n",
    "    # Generate standoff data for the new train and val datasets\n",
    "    generate_standoff_data(new_train_data, llt_dict, llt_to_pt, pt_dict, os.path.join(dst, \"train\"))\n",
    "    generate_standoff_data(new_val_data, llt_dict, llt_to_pt, pt_dict, os.path.join(dst, \"val\"))\n",
    "\n",
    "    # Process Dev dataset as test\n",
    "    merged_dev_data = merge_tweets_spans(dev_tweets, dev_spans)\n",
    "    generate_standoff_data(merged_dev_data, llt_dict, llt_to_pt, pt_dict, os.path.join(dst, \"test\"))\n",
    "\n",
    "    # Process test data for inference\n",
    "    test_src = \"./data/smm4h23/Task5_test\"\n",
    "    test_tweets = read_tweet_tsv_file(os.path.join(test_src, \"tweets.tsv\"))\n",
    "    generate_test_data(test_tweets, os.path.join(dst, \"infer\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MultiBioNER",
   "language": "python",
   "name": "multibioner"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
