{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c4c7bf-089d-4561-921d-31f6070e6852",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def load_xml_files_from_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Load and parse all XML files in a specified folder.\n",
    "\n",
    "    Args:\n",
    "    folder_path (str): The path to the folder containing XML files.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are filenames and values are the parsed XML root elements.\n",
    "    \"\"\"\n",
    "    xml_files = {}\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.xml'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            try:\n",
    "                tree = ET.parse(file_path)\n",
    "                root = tree.getroot()\n",
    "                xml_files[filename] = root\n",
    "            except ET.ParseError as e:\n",
    "                print(str(e))\n",
    "                xml_files[filename] = str(e)\n",
    "    return xml_files\n",
    "\n",
    "def create_standoff_files_with_section_text(xml_files, base_output_folder, split_type, ignore_discontinuous=False):\n",
    "    \"\"\"\n",
    "    Create .txt and .ann files for each section in each XML file, handling multiple and discontinuous start positions,\n",
    "    and filtering for specific entity types, including normalization annotations.\n",
    "    \"\"\"\n",
    "    for file_name, root in xml_files.items():\n",
    "        base_filename = os.path.splitext(file_name)[0]\n",
    "        section_id_to_name = {section.get('id'): section.get('name') for section in root.findall('.//Section')}\n",
    "        sections_text = {section.get('id'): section.text for section in root.findall('.//Section')}\n",
    "        mentions = root.findall('.//Mention')\n",
    "\n",
    "        # Create normalization map\n",
    "        normalization_map = {}\n",
    "        for reaction in root.findall('.//Reaction'):\n",
    "            reaction_str = reaction.get('str').lower().strip()\n",
    "            normalization = reaction.find('.//Normalization')\n",
    "            if normalization is not None:\n",
    "                meddra_pt_id = normalization.get('meddra_pt_id')\n",
    "                meddra_pt = normalization.get('meddra_pt')\n",
    "                meddra_llt_id = normalization.get('meddra_llt_id', None)\n",
    "                meddra_llt = normalization.get('meddra_llt', None)\n",
    "                normalization_map[reaction_str] = (meddra_pt_id, meddra_pt, meddra_llt_id, meddra_llt)\n",
    "\n",
    "        if not mentions:\n",
    "            continue\n",
    "\n",
    "        sections = {}\n",
    "\n",
    "        for mention in mentions:\n",
    "            mention_type = mention.get('type')\n",
    "\n",
    "            if mention_type not in ['AdverseReaction']:\n",
    "                continue\n",
    "\n",
    "            section_id = mention.get('section')\n",
    "            section_name = section_id_to_name.get(section_id, \"UnknownSection\")\n",
    "            start_positions = mention.get('start').split(',')\n",
    "            lengths = mention.get('len').split(',')\n",
    "            mention_text = mention.get('str')\n",
    "\n",
    "            if len(start_positions) != len(lengths):\n",
    "                print(f\"Warning: Mismatch in lengths and starts in mention '{mention.get('id')}'\")\n",
    "                continue\n",
    "\n",
    "            if len(start_positions) > 1 and ignore_discontinuous:\n",
    "                continue\n",
    "\n",
    "            if section_id not in sections:\n",
    "                sections[section_id] = {'text': sections_text.get(section_id, \"\"), 'mentions': [], 'name': section_name}\n",
    "\n",
    "            mention_ranges = []\n",
    "            for start, length in zip(start_positions, lengths):\n",
    "                start = int(start)\n",
    "                length = int(length)\n",
    "                end = start + length\n",
    "                mention_ranges.append((start, end))\n",
    "\n",
    "            formatted_ranges = ';'.join([f\"{start} {end}\" for start, end in mention_ranges])\n",
    "            true_text = \" \".join([sections_text[section_id][a:b] for a,b in mention_ranges])\n",
    "            sections[section_id]['mentions'].append((\"ADE\", formatted_ranges, true_text, mention_text.lower().strip() in normalization_map, mention_text.lower().strip()))\n",
    "\n",
    "        for section_id, data in sections.items():\n",
    "            safe_section_name = ''.join(e for e in data['name'] if e.isalnum())\n",
    "            section_output_folder = os.path.join(base_output_folder, safe_section_name, split_type)\n",
    "            os.makedirs(section_output_folder, exist_ok=True)\n",
    "\n",
    "            txt_filename = os.path.join(section_output_folder, f\"{base_filename}_{safe_section_name}.txt\")\n",
    "            ann_filename = os.path.join(section_output_folder, f\"{base_filename}_{safe_section_name}.ann\")\n",
    "\n",
    "            text_bound_annotations = []\n",
    "            normalization_annotations = []\n",
    "\n",
    "            with open(txt_filename, 'w') as txt_file, open(ann_filename, 'w') as ann_file:\n",
    "                txt_file.write(data['text'])\n",
    "                mention_counter = 1\n",
    "                normalization_counter = 1\n",
    "                data['mentions'].sort(key=lambda x: int(x[1].split(';')[0].split()[0]))\n",
    "                \n",
    "                for mention_type, mention_ranges, text, has_normalization, mention_text in data['mentions']:\n",
    "                    if has_normalization:\n",
    "                        text_bound_annotation = f\"T{mention_counter}\\t{mention_type} {mention_ranges}\\t{text}\\n\"\n",
    "                        text_bound_annotations.append(text_bound_annotation)\n",
    "\n",
    "                        meddra_pt_id, meddra_pt, meddra_llt_id, meddra_llt = normalization_map[mention_text]\n",
    "                        if meddra_pt_id and meddra_pt:\n",
    "                            normalization_annotation = f\"N{normalization_counter}\\tReference T{mention_counter} meddra_pt:{meddra_pt_id}\\t{meddra_pt}\\n\"\n",
    "                            normalization_annotations.append(normalization_annotation)\n",
    "                            normalization_counter += 1\n",
    "                        if meddra_llt_id and meddra_llt:\n",
    "                            normalization_annotation = f\"N{normalization_counter}\\tReference T{mention_counter} meddra_llt:{meddra_llt_id}\\t{meddra_llt}\\n\"\n",
    "                            normalization_annotations.append(normalization_annotation)\n",
    "                            normalization_counter += 1\n",
    "                        mention_counter += 1\n",
    "\n",
    "                # Write all text-bound annotations first\n",
    "                for t_annotation in text_bound_annotations:\n",
    "                    ann_file.write(t_annotation)\n",
    "\n",
    "                # Followed by all normalization annotations\n",
    "                for n_annotation in normalization_annotations:\n",
    "                    ann_file.write(n_annotation)\n",
    "\n",
    "# Specify the folder path (for this example, we use the same folder as the provided XML files)\n",
    "folder_path = './data/TAC2017/train_xml'\n",
    "\n",
    "# Load and parse all XML files in the specified folder\n",
    "loaded_xml_files = load_xml_files_from_folder(folder_path)\n",
    "\n",
    "import random\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Convert the dictionary items into a list of (filename, root) tuples\n",
    "xml_items = list(loaded_xml_files.items())\n",
    "\n",
    "# Shuffle the list randomly\n",
    "random.shuffle(xml_items)\n",
    "\n",
    "# Calculate the split point\n",
    "split_index = int(0.9 * len(xml_items))\n",
    "\n",
    "# Split the list into train and validation sets\n",
    "train_xml_files = dict(xml_items[:split_index])\n",
    "val_xml_files = dict(xml_items[split_index:])\n",
    "\n",
    "# Print the number of files in each set\n",
    "print(f\"Number of files in train set: {len(train_xml_files)}\")\n",
    "print(f\"Number of files in validation set: {len(val_xml_files)}\")\n",
    "\n",
    "# Specify the folder path (for this example, we use the same folder as the provided XML files)\n",
    "folder_path = './data/TAC2017/gold_xml'\n",
    "\n",
    "# Load and parse all XML files in the specified folder\n",
    "test_xml_files = load_xml_files_from_folder(folder_path)\n",
    "print(f\"Number of files in test set: {len(test_xml_files)}\")\n",
    "\n",
    "base_output_folder = './data/TAC2017/standoff/ADEs_only'\n",
    "\n",
    "create_standoff_files_with_section_text(train_xml_files, base_output_folder, 'train', ignore_discontinuous=True)\n",
    "create_standoff_files_with_section_text(val_xml_files, base_output_folder, 'val', ignore_discontinuous=False)\n",
    "create_standoff_files_with_section_text(test_xml_files, base_output_folder, 'test', ignore_discontinuous=False)\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def merge_folders(src1, src2, src3, dest):\n",
    "    \"\"\"Merges the 'train', 'val', and 'test' subdirectories of three source directories into a single directory at the specified destination.\"\"\"\n",
    "\n",
    "    subdirs = ['train', 'val', 'test']\n",
    "\n",
    "    for src in (src1, src2, src3):\n",
    "        if not os.path.isdir(src):\n",
    "            raise ValueError(f\"Source directory '{src}' does not exist.\")\n",
    "\n",
    "        for subdir in subdirs:\n",
    "            target = os.path.join(dest, subdir)\n",
    "\n",
    "            if not os.path.exists(target):\n",
    "                os.makedirs(target)\n",
    "\n",
    "            src_subdir = os.path.join(src, subdir)\n",
    "            if os.path.isdir(src_subdir):\n",
    "                for dirpath, _, filenames in os.walk(src_subdir):\n",
    "                    for filename in filenames:\n",
    "                        if not filename.startswith(\".\"):\n",
    "                            src_file = os.path.join(dirpath, filename)\n",
    "                            dst_file = os.path.join(target, filename)\n",
    "                            if not os.path.exists(dst_file):\n",
    "                                shutil.copy2(src_file, dst_file)\n",
    "                            else:\n",
    "                                print(f\"Warning: file {filename} already exists at destination; skipping...\")\n",
    "\n",
    "merge_folders('./data/TAC2017/standoff/ADEs_only/adversereactions', './data/TAC2017/standoff/ADEs_only/boxedwarnings', './data/TAC2017/standoff/ADEs_only/warningsandprecautions', './data/TAC2017/standoff/ADEs_only/tac_ad_bo_wa')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RECOT",
   "language": "python",
   "name": "recot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
